{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70da1806-2db8-46b9-9dee-d1f9c6ad7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc53bfd-dc5b-4e34-8aef-c14049b1678c",
   "metadata": {},
   "source": [
    "## \"All of Statistics: A Concise Course in Statistical Inference\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaaf98c-88ae-4a41-ab2b-185f9f6db325",
   "metadata": {},
   "source": [
    "### Chapter 10: Hypothesis Testing and p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918bb89-242d-4f19-aac8-b1efc72b336c",
   "metadata": {},
   "source": [
    "**Problem 7** In 1861, 10 essays appeared in the *New Orleans Daily Crescent*. They  were  signed by \"Quintus Curtius  Snodgrass\" and some people suspected they were actually written by Mark Twain. To investigate this, we will consider the proportion of three letter words found in an author's work. From eight Twain essays we have:\n",
    "\n",
    ".225 .262 .217 .240 .230 .229 .235 .217\n",
    "\n",
    "From 10 Sodgrass essays we have :\n",
    "\n",
    ".209 .205 .196 .210 .202 .207 .224 .223 .220 .201\n",
    "\n",
    "(a) Perform a Wald test for equality of the means. Use the nonparametric plug-in estimator. Report the p-value and the 95 percent confidence interval for the difference in means. What do you conclude?\n",
    "\n",
    "(b) Now use permutation test to avoid the use of large sample methods. What is your conclusion? (Brinegar (1963))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113554bf-f291-4f33-905a-cc22d2d4eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "twain = np.array([.225, .262, .217, .240, .230, .229, .235, .217])\n",
    "snodgrass  =  np.array([.209, .205, .196, .210, .202, .207, .224, .223, .220, .201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c531bc-7665-4097-9127-933d15e0188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twain mean = 0.231875\n",
      "Snodgrasss mean =0.2097\n",
      "Difference =0.022175\n",
      "p-value = 0.0002\n",
      "95% confidence interval = (0.010, 0.034)\n"
     ]
    }
   ],
   "source": [
    "# Nonparametric plug-in estimator simply the difference in the sample means.\n",
    "\n",
    "est = twain.mean() - snodgrass.mean() # plug-in estimator (difference of sample means)\n",
    "\n",
    "se = np.sqrt(twain.var(ddof=1)/twain.size + snodgrass.var(ddof=1)/snodgrass.size) # estimated standard error ussing sample variances\n",
    "\n",
    "W = abs(est / se)\n",
    "\n",
    "print(f'Twain mean = {twain.mean()}')\n",
    "print(f'Snodgrasss mean ={snodgrass.mean()}')\n",
    "print(f'Difference ={twain.mean() - snodgrass.mean()}')\n",
    "print(f'p-value = {2*st.norm.cdf(-W):.4f}')\n",
    "print(f'95% confidence interval = ({est-1.96*se:.3f}, {est+1.96*se:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5765e-d6df-4e82-8620-4004318985ea",
   "metadata": {},
   "source": [
    "There is a statistically very significant difference between the sample means. There seems to be a measurable difference in the writing style, although the books could still be written by the same person, just in a different style. We would need to control for other factors like the subject of the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6aea174-3e88-4492-baa9-b15d5d56d0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000779\n"
     ]
    }
   ],
   "source": [
    "# Permutation test\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "est = twain.mean() - snodgrass.mean()\n",
    "\n",
    "pooled = np.concatenate([twain, snodgrass])\n",
    "\n",
    "count = 0\n",
    "\n",
    "reps = 10**6\n",
    "\n",
    "for _ in range(reps): # Better to use the vectorized versions below\n",
    "    permuted = rng.permuted(pooled)\n",
    "    count += abs(permuted[:twain.size].mean() - permuted[twain.size:].mean()) >= est\n",
    "    \n",
    "print(count / reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195b740f-6585-4529-beed-138c87de3804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000721"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorized version (much faster!)\n",
    "\n",
    "reindex = np.random.rand(10**6, pooled.size).argsort(axis=1) # A bit of a wasteful hack but faster in this example\n",
    "\n",
    "(np.abs(pooled[reindex][:,:twain.size].mean(axis=1) - pooled[reindex][:,twain.size:].mean(axis=1)) > est).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc758679-a38a-4876-90a1-f1ecb62314b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000726"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another vectorized version that it better for longer arrays (but not quite as fast for this length 18 array)\n",
    "# For longer arrays the argsort hack is costly because it needs to (implicitly) sort all the arrays\n",
    "\n",
    "reps = 10**6\n",
    "\n",
    "permutations = rng.permuted(np.stack((pooled,)*reps), axis=-1)\n",
    "\n",
    "(np.abs(permutations[:,:twain.size].mean(axis=1) - permutations[:,twain.size:].mean(axis=1)) > est).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e35def3-7d1a-4c1c-9beb-c16f6e553909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000709"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoiding the count operation in the loop already speeds things up quite a bit\n",
    "\n",
    "reps = 10**6\n",
    "permutations = []\n",
    "for _ in range(reps): # Avoiding the count increase in the loop makes it much faster\n",
    "    permutations.append(rng.permuted(pooled))\n",
    "\n",
    "permutations_array = np.array(permutations)\n",
    "\n",
    "(np.abs(permutations_array[:,:twain.size].mean(axis=1) - permutations_array[:,twain.size:].mean(axis=1)) > est).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5336a1e7-db36-4353-adcd-ff04aced84eb",
   "metadata": {},
   "source": [
    "The p-value from the permutation test is around 0.0007, over three times larger than the p-value from the Wald test above, but still very small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6bb67d-7763-4072-b0e7-970b76902edf",
   "metadata": {},
   "source": [
    "**Problem 10** Here are the number of elderly Jewish and Chinese women who died just before and after the Chinese Harvest Moon Festival.\n",
    "\n",
    "Week | Chinese | Jewish\n",
    "-- | -- | --\n",
    "-2 | 55 | 141\n",
    "-1 | 33 | 145\n",
    "1 | 70 | 139\n",
    "2 | 49 | 161\n",
    "\n",
    "Compare the two mortality patterns. (Phillips and Smith (1990))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d055c7f-6440-47dd-a919-0c1c286de9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese = np.array([55, 33, 70, 49])\n",
    "jewish = np.array([141, 145, 139, 161])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59f358e1-cd67-4e8d-aea3-8312e9ffc6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.365476186680414e-05"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the deaths in weeks -1 and 1\n",
    "\n",
    "before = chinese[1]\n",
    "total = chinese[[1, 2]].sum()\n",
    "\n",
    "est = before / total\n",
    "se = np.sqrt(est * (1 - est) / total)\n",
    "\n",
    "2 * st.norm.cdf(-abs(est - 1/2) / se) # p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29788053-819b-4471-a8c5-414ba1cb4a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7217552078897358"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the deaths in weeks -1 and 1\n",
    "\n",
    "before = jewish[1]\n",
    "total = jewish[[1, 2]].sum()\n",
    "\n",
    "est = before / total\n",
    "se = np.sqrt(est * (1 - est) / total)\n",
    "\n",
    "2 * st.norm.cdf(-abs(est - 1/2) / se) # p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73b645ef-ac52-4fe6-954c-a24eafe84541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5556399329877082"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the deaths in weeks  -2 and 2\n",
    "\n",
    "before = chinese[0]\n",
    "total = chinese[[0, 3]].sum()\n",
    "\n",
    "est = before / total\n",
    "se = np.sqrt(est * (1 - est) / total)\n",
    "\n",
    "2 * st.norm.cdf(-abs(est - 1/2) / se) # p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6ed29b6-55e5-42c1-87d5-c83b89395e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24874511916003872"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the deaths in weeks  -2 and 2\n",
    "\n",
    "before = jewish[0]\n",
    "total = jewish[[0, 3]].sum()\n",
    "\n",
    "est = before / total\n",
    "se = np.sqrt(est * (1 - est) / total)\n",
    "\n",
    "2 * st.norm.cdf(-abs(est - 1/2) / se)  # p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0f2e5-2c41-4198-a2ad-4689c9ff700f",
   "metadata": {},
   "source": [
    "The only significant difference was between weeks -1 and 1 in chinese women. That difference was very significant with a p-value of 0.00009 and a significant effect too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572e771-8649-4a9b-acd1-3b5e790edf39",
   "metadata": {},
   "source": [
    "**Problem 11** A randomized, double-blind experiment was conducted to assess the effectiveness of several drugs for reducing postoperative nausea. The data are as follows.\n",
    "\n",
    "| | Number of patients | Incidence of Nausea\n",
    "-- | -- | --\n",
    "Placebo | 80 | 45\n",
    "Chlorpromazine | 75 | 26\n",
    "Dimenhydrinate | 85 | 52\n",
    "Pentobarbital (100 mg) | 67 | 35\n",
    "Pentobarbital (150 mg) | 85 | 37\n",
    "\n",
    "(a) Test each drug versus the placebo at the 5 per cent level. Also, report the estimated odds-ratios. Summarize your findings.\n",
    "\n",
    "(b) Use the Bonferroni and the FDR method to adjust for multiple testing. (Beecher (1959))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38449b0-6c53-47ae-91d5-131b4ce9d7dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can model the incidence of nausea for each group as a binomial random variable. The null hypothesis is that the distributions are equal, i.e. have the same $p$ parameter.\n",
    "\n",
    "- The book probably expects us to use the Wald test with $\\theta = p_1 - p_2$ (standard error could be from MLE or nonparametric plug-in estimate, probably the former is better since the MLE is asymptotically optimal).\n",
    "- Could use the t-test to compare the means (estimated $\\hat p$ in this case) of each group with the placebo group. More precise for small samples.\n",
    "- Exact tests would be binoimial test or Fisher exact test.\n",
    "- Likelihood ratio test for $\\theta = p_1 - p_2$.\n",
    "\n",
    "Estimated odds are $\\hat p_1 / (1 - \\hat p_1)$ and $\\hat p_2 / (1 - \\hat p_2)$. The odds ratio is simply the ratio of those two estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff4d0525-140f-4799-be95-29ce083ddb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'Number': [80, 75, 85, 67, 85],\n",
    "    'Incidence': [45, 26, 52, 35, 37]\n",
    "    },\n",
    "    index = ['Placebo', 'Chlorpromazine', 'Dimenhydrate', 'Pentobarbital (100 mg)', 'Pentobarbital (150 mg)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e8842ee-eb71-4fe0-bbab-a4b14268a0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta hat</th>\n",
       "      <th>odds ratio</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chlorpromazine</th>\n",
       "      <td>0.215833</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>0.005703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dimenhydrate</th>\n",
       "      <td>-0.049265</td>\n",
       "      <td>1.225589</td>\n",
       "      <td>0.520232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pentobarbital (100 mg)</th>\n",
       "      <td>0.040112</td>\n",
       "      <td>0.850694</td>\n",
       "      <td>0.626664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pentobarbital (150 mg)</th>\n",
       "      <td>0.127206</td>\n",
       "      <td>0.599537</td>\n",
       "      <td>0.099639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        theta hat  odds ratio   p-value\n",
       "Chlorpromazine           0.215833    0.412698  0.005703\n",
       "Dimenhydrate            -0.049265    1.225589  0.520232\n",
       "Pentobarbital (100 mg)   0.040112    0.850694  0.626664\n",
       "Pentobarbital (150 mg)   0.127206    0.599537  0.099639"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wald test\n",
    "\n",
    "df['p hat'] = df['Incidence'] / df['Number']\n",
    "df['var'] = df['p hat'] * (1 - df['p hat']) / df['Number']\n",
    "\n",
    "df['theta hat'] = df.loc['Placebo']['p hat'] - df['p hat']\n",
    "df['se'] = np.sqrt(df['var']  + df.loc['Placebo']['var'])\n",
    "\n",
    "df['W'] = np.abs(df['theta hat'] / df['se'])\n",
    "df['p-value'] = 2 * st.norm.cdf(-df['W'])\n",
    "\n",
    "df['odds'] = df['p hat'] / (1 - df['p hat'])\n",
    "df['odds ratio'] = df['odds'] / df.loc['Placebo']['odds']\n",
    "\n",
    "df[['theta hat','odds ratio', 'p-value']][df.index != 'Placebo']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972b119-14d5-45dc-8dac-8e1fa85763fe",
   "metadata": {},
   "source": [
    "At a 5% level only the Chlorpromazine shows a significant difference. The odds ratio is 0.4, which means that the odds are estimated at double those of the placebo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e2561-f655-487d-a87a-a838f2e057e0",
   "metadata": {},
   "source": [
    "Now we apply the Bonferroni Method and the Benjamini-Hochberg Method (aka FDR method).\n",
    "\n",
    "Because we did four tests, the Bonferroni method divides the level by 4. Even then Chlorpromazine shows a significant difference, since the p-value is $0.006$, which is less than $0.05/4$.\n",
    "\n",
    "Benjamini-Hochberg:\n",
    "- Order the p-values 0.006, 0.1, 0.5, 0.6\n",
    "- Find greatest $P_{(i)}$ which is less than $i/4\\cdot0.05$, in this case $0.006$\n",
    "- Reject the hypotheses for $P_{(i)}$ and all p-values below and fail to reject the rest.\n",
    "\n",
    "We can adjusst the p-values with the Bonferroni method, by multiplying by the number of tests, in this case $4$. The BH method also gives a way to adjust the p-values. The p-value 0.006 would be adjusted to $0.006 \\cdot 4/1 = 0.0015$, which happens to be the same as the Bonferroni adjustment.\n",
    "\n",
    "With both methods, Chlorpromazine shows a significant effect, with adjusted p-value $0.0015$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba7e77-fb4f-4fc6-97c9-97560c710350",
   "metadata": {},
   "source": [
    "**Problem 12** Let $X_1,\\dots,X_n \\sim \\text{Poisson}(\\lambda)$.\n",
    "\n",
    "(a) Let $\\lambda_0 > 0$. Find the size $\\alpha$ Wald test for $H_0:\\lambda=\\lambda_0$ versus $H_1:\\lambda\\ne\\lambda_0$.\n",
    "\n",
    "(b) Let $\\lambda_0=1$, $n=20$ and $\\alpha=.05$. Simulate $X_1,\\dots,X_n \\sim \\text{Poisson}(\\lambda_0)$ ans perform the Wald test. Repeat many times and count how often you reject the null. How close is the type I error rate to 0.05?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42a68e-7bd5-4a45-a5b2-cfad88589ea3",
   "metadata": {},
   "source": [
    "Part (a):\n",
    "\n",
    "From Exercise 5 in Chapter 9: $\\hat\\lambda = \\bar X_n$ and $\\hat{\\text{se}}  =  \\sqrt{\\hat\\lambda/n} =  \\sqrt{\\bar X_n/n}$.\n",
    "\n",
    "The size is given by $$\\mathbb{P}_{\\lambda_0}\\left(\\left|\\frac{\\hat\\lambda - \\lambda_0}{\\hat{\\text{se}}}\\right| \\ge z_{\\alpha/2}\\right) = \\mathbb{P}_{\\lambda_0}\\left(\\left|\\frac{\\bar X_n - \\lambda_0}{\\sqrt{\\bar X_n/n}}\\right| \\ge z_{\\alpha/2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d55f0913-c57f-49fc-b092-84de67b3e794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052652"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda0 = 1.\n",
    "size = 20\n",
    "alpha = 0.05\n",
    "rv = st.poisson(lambda0)\n",
    "z = st.norm.ppf(1 - alpha/2)\n",
    "reps =  10**6\n",
    "\n",
    "sample  = rv.rvs(size=size*reps).reshape(reps, size)\n",
    "lambda_hat = sample.mean(axis=1)\n",
    "rejections = np.sum(np.abs((lambda_hat - lambda0) * np.sqrt(size / lambda_hat)) > z)\n",
    "\n",
    "rejections / reps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
